{
  "version":"2.0",
  "metadata":{
    "apiVersion":"2023-09-30",
    "endpointPrefix":"bedrock-runtime",
    "jsonVersion":"1.1",
    "protocol":"rest-json",
    "serviceFullName":"Amazon Bedrock Runtime",
    "serviceId":"Bedrock Runtime",
    "signatureVersion":"v4",
    "signingName":"bedrock",
    "uid":"bedrock-runtime-2023-09-30"
  },
  "operations":{
    "InvokeModel":{
      "name":"InvokeModel",
      "http":{
        "method":"POST",
        "requestUri":"/model/{modelId}/invoke",
        "responseCode":200
      },
      "input":{"shape":"InvokeModelRequest"},
      "output":{"shape":"InvokeModelResponse"},
      "errors":[
        {"shape":"AccessDeniedException"},
        {"shape":"ResourceNotFoundException"},
        {"shape":"ThrottlingException"},
        {"shape":"ModelTimeoutException"},
        {"shape":"InternalServerException"},
        {"shape":"ValidationException"},
        {"shape":"ModelNotReadyException"},
        {"shape":"ServiceQuotaExceededException"},
        {"shape":"ModelErrorException"}
      ],
      "documentation":"<p>Invokes the specified Amazon Bedrock model to run inference using the prompt and inference parameters provided in the request body. You use model inference to generate text, images, and embeddings.</p> <p>For example code, see <i>Invoke model code examples</i> in the <i>Amazon Bedrock User Guide</i>. </p> <p>This operation requires permission for the <code>bedrock:InvokeModel</code> action.</p>"
    },
    "InvokeModelWithResponseStream":{
      "name":"InvokeModelWithResponseStream",
      "http":{
        "method":"POST",
        "requestUri":"/model/{modelId}/invoke-with-response-stream",
        "responseCode":200
      },
      "input":{"shape":"InvokeModelWithResponseStreamRequest"},
      "output":{"shape":"InvokeModelWithResponseStreamResponse"},
      "errors":[
        {"shape":"AccessDeniedException"},
        {"shape":"ResourceNotFoundException"},
        {"shape":"ThrottlingException"},
        {"shape":"ModelTimeoutException"},
        {"shape":"InternalServerException"},
        {"shape":"ModelStreamErrorException"},
        {"shape":"ValidationException"},
        {"shape":"ModelNotReadyException"},
        {"shape":"ServiceQuotaExceededException"},
        {"shape":"ModelErrorException"}
      ],
      "documentation":"<p>Invoke the specified Amazon Bedrock model to run inference using the prompt and inference parameters provided in the request body. The response is returned in a stream.</p> <p>To see if a model supports streaming, call <a href=\"https://docs.aws.amazon.com/bedrock/latest/APIReference/API_GetFoundationModel.html\">GetFoundationModel</a> and check the <code>responseStreamingSupported</code> field in the response.</p> <note> <p>The CLI doesn't support <code>InvokeModelWithResponseStream</code>.</p> </note> <p>For example code, see <i>Invoke model with streaming code example</i> in the <i>Amazon Bedrock User Guide</i>. </p> <p>This operation requires permissions to perform the <code>bedrock:InvokeModelWithResponseStream</code> action. </p>"
    }
  },
  "shapes":{
    "AccessDeniedException":{
      "type":"structure",
      "members":{
        "message":{"shape":"NonBlankString"}
      },
      "documentation":"<p>The request is denied because of missing access permissions.</p>",
      "error":{
        "httpStatusCode":403,
        "senderFault":true
      },
      "exception":true
    },
    "Body":{
      "type":"blob",
      "max":25000000,
      "min":0,
      "sensitive":true
    },
    "GuardrailIdentifier":{
      "type":"string",
      "max":2048,
      "min":0,
      "pattern":"(([a-z0-9]+)|(arn:aws(-[^:]+)?:bedrock:[a-z0-9-]{1,20}:[0-9]{12}:guardrail/[a-z0-9]+))"
    },
    "GuardrailVersion":{
      "type":"string",
      "pattern":"(([1-9][0-9]{0,7})|(DRAFT))"
    },
    "InternalServerException":{
      "type":"structure",
      "members":{
        "message":{"shape":"NonBlankString"}
      },
      "documentation":"<p>An internal server error occurred. Retry your request.</p>",
      "error":{"httpStatusCode":500},
      "exception":true,
      "fault":true
    },
    "InvokeModelIdentifier":{
      "type":"string",
      "max":2048,
      "min":1,
      "pattern":"(arn:aws(-[^:]+)?:bedrock:[a-z0-9-]{1,20}:(([0-9]{12}:custom-model/[a-z0-9-]{1,63}[.]{1}[a-z0-9-]{1,63}/[a-z0-9]{12})|(:foundation-model/[a-z0-9-]{1,63}[.]{1}[a-z0-9-]{1,63}([.:]?[a-z0-9-]{1,63}))|([0-9]{12}:provisioned-model/[a-z0-9]{12})))|([a-z0-9-]{1,63}[.]{1}[a-z0-9-]{1,63}([.:]?[a-z0-9-]{1,63}))|(([0-9a-zA-Z][_-]?)+)"
    },
    "InvokeModelRequest":{
      "type":"structure",
      "required":[
        "body",
        "modelId"
      ],
      "members":{
        "body":{
          "shape":"Body",
          "documentation":"<p>The prompt and inference parameters in the format specified in the <code>contentType</code> in the header. To see the format and content of the request and response bodies for different models, refer to <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html\">Inference parameters</a>. For more information, see <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/api-methods-run.html\">Run inference</a> in the Bedrock User Guide.</p>"
        },
        "contentType":{
          "shape":"MimeType",
          "documentation":"<p>The MIME type of the input data in the request. The default value is <code>application/json</code>.</p>",
          "location":"header",
          "locationName":"Content-Type"
        },
        "accept":{
          "shape":"MimeType",
          "documentation":"<p>The desired MIME type of the inference body in the response. The default value is <code>application/json</code>.</p>",
          "location":"header",
          "locationName":"Accept"
        },
        "modelId":{
          "shape":"InvokeModelIdentifier",
          "documentation":"<p>The unique identifier of the model to invoke to run inference.</p> <p>The <code>modelId</code> to provide depends on the type of model that you use:</p> <ul> <li> <p>If you use a base model, specify the model ID or its ARN. For a list of model IDs for base models, see <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html#model-ids-arns\">Amazon Bedrock base model IDs (on-demand throughput)</a> in the Amazon Bedrock User Guide.</p> </li> <li> <p>If you use a provisioned model, specify the ARN of the Provisioned Throughput. For more information, see <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/prov-thru-use.html\">Run inference using a Provisioned Throughput</a> in the Amazon Bedrock User Guide.</p> </li> <li> <p>If you use a custom model, first purchase Provisioned Throughput for it. Then specify the ARN of the resulting provisioned model. For more information, see <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-use.html\">Use a custom model in Amazon Bedrock</a> in the Amazon Bedrock User Guide.</p> </li> </ul>",
          "location":"uri",
          "locationName":"modelId"
        },
        "trace":{
          "shape":"Trace",
          "documentation":"<p>Specifies whether to enable or disable the Bedrock trace. If enabled, you can see the full Bedrock trace.</p>",
          "location":"header",
          "locationName":"X-Amzn-Bedrock-Trace"
        },
        "guardrailIdentifier":{
          "shape":"GuardrailIdentifier",
          "documentation":"<p>The unique identifier of the guardrail that you want to use. If you don't provide a value, no guardrail is applied to the invocation.</p> <p>An error will be thrown in the following situations.</p> <ul> <li> <p>You don't provide a guardrail identifier but you specify the <code>amazon-bedrock-guardrailConfig</code> field in the request body.</p> </li> <li> <p>You enable the guardrail but the <code>contentType</code> isn't <code>application/json</code>.</p> </li> <li> <p>You provide a guardrail identifier, but <code>guardrailVersion</code> isn't specified.</p> </li> </ul>",
          "location":"header",
          "locationName":"X-Amzn-Bedrock-GuardrailIdentifier"
        },
        "guardrailVersion":{
          "shape":"GuardrailVersion",
          "documentation":"<p>The version number for the guardrail. The value can also be <code>DRAFT</code>.</p>",
          "location":"header",
          "locationName":"X-Amzn-Bedrock-GuardrailVersion"
        }
      },
      "payload":"body"
    },
    "InvokeModelResponse":{
      "type":"structure",
      "required":[
        "body",
        "contentType"
      ],
      "members":{
        "body":{
          "shape":"Body",
          "documentation":"<p>Inference response from the model in the format specified in the <code>contentType</code> header. To see the format and content of the request and response bodies for different models, refer to <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html\">Inference parameters</a>.</p>"
        },
        "contentType":{
          "shape":"MimeType",
          "documentation":"<p>The MIME type of the inference result.</p>",
          "location":"header",
          "locationName":"Content-Type"
        }
      },
      "payload":"body"
    },
    "InvokeModelWithResponseStreamRequest":{
      "type":"structure",
      "required":[
        "body",
        "modelId"
      ],
      "members":{
        "body":{
          "shape":"Body",
          "documentation":"<p>The prompt and inference parameters in the format specified in the <code>contentType</code> in the header. To see the format and content of the request and response bodies for different models, refer to <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html\">Inference parameters</a>. For more information, see <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/api-methods-run.html\">Run inference</a> in the Bedrock User Guide.</p>"
        },
        "contentType":{
          "shape":"MimeType",
          "documentation":"<p>The MIME type of the input data in the request. The default value is <code>application/json</code>.</p>",
          "location":"header",
          "locationName":"Content-Type"
        },
        "accept":{
          "shape":"MimeType",
          "documentation":"<p>The desired MIME type of the inference body in the response. The default value is <code>application/json</code>.</p>",
          "location":"header",
          "locationName":"X-Amzn-Bedrock-Accept"
        },
        "modelId":{
          "shape":"InvokeModelIdentifier",
          "documentation":"<p>The unique identifier of the model to invoke to run inference.</p> <p>The <code>modelId</code> to provide depends on the type of model that you use:</p> <ul> <li> <p>If you use a base model, specify the model ID or its ARN. For a list of model IDs for base models, see <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html#model-ids-arns\">Amazon Bedrock base model IDs (on-demand throughput)</a> in the Amazon Bedrock User Guide.</p> </li> <li> <p>If you use a provisioned model, specify the ARN of the Provisioned Throughput. For more information, see <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/prov-thru-use.html\">Run inference using a Provisioned Throughput</a> in the Amazon Bedrock User Guide.</p> </li> <li> <p>If you use a custom model, first purchase Provisioned Throughput for it. Then specify the ARN of the resulting provisioned model. For more information, see <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-use.html\">Use a custom model in Amazon Bedrock</a> in the Amazon Bedrock User Guide.</p> </li> </ul>",
          "location":"uri",
          "locationName":"modelId"
        },
        "trace":{
          "shape":"Trace",
          "documentation":"<p>Specifies whether to enable or disable the Bedrock trace. If enabled, you can see the full Bedrock trace.</p>",
          "location":"header",
          "locationName":"X-Amzn-Bedrock-Trace"
        },
        "guardrailIdentifier":{
          "shape":"GuardrailIdentifier",
          "documentation":"<p>The unique identifier of the guardrail that you want to use. If you don't provide a value, no guardrail is applied to the invocation.</p> <p>An error is thrown in the following situations.</p> <ul> <li> <p>You don't provide a guardrail identifier but you specify the <code>amazon-bedrock-guardrailConfig</code> field in the request body.</p> </li> <li> <p>You enable the guardrail but the <code>contentType</code> isn't <code>application/json</code>.</p> </li> <li> <p>You provide a guardrail identifier, but <code>guardrailVersion</code> isn't specified.</p> </li> </ul>",
          "location":"header",
          "locationName":"X-Amzn-Bedrock-GuardrailIdentifier"
        },
        "guardrailVersion":{
          "shape":"GuardrailVersion",
          "documentation":"<p>The version number for the guardrail. The value can also be <code>DRAFT</code>.</p>",
          "location":"header",
          "locationName":"X-Amzn-Bedrock-GuardrailVersion"
        }
      },
      "payload":"body"
    },
    "InvokeModelWithResponseStreamResponse":{
      "type":"structure",
      "required":[
        "body",
        "contentType"
      ],
      "members":{
        "body":{
          "shape":"ResponseStream",
          "documentation":"<p>Inference response from the model in the format specified by the <code>contentType</code> header. To see the format and content of this field for different models, refer to <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html\">Inference parameters</a>.</p>"
        },
        "contentType":{
          "shape":"MimeType",
          "documentation":"<p>The MIME type of the inference result.</p>",
          "location":"header",
          "locationName":"X-Amzn-Bedrock-Content-Type"
        }
      },
      "payload":"body"
    },
    "MimeType":{"type":"string"},
    "ModelErrorException":{
      "type":"structure",
      "members":{
        "message":{"shape":"NonBlankString"},
        "originalStatusCode":{
          "shape":"StatusCode",
          "documentation":"<p>The original status code.</p>"
        },
        "resourceName":{
          "shape":"NonBlankString",
          "documentation":"<p>The resource name.</p>"
        }
      },
      "documentation":"<p>The request failed due to an error while processing the model.</p>",
      "error":{
        "httpStatusCode":424,
        "senderFault":true
      },
      "exception":true
    },
    "ModelNotReadyException":{
      "type":"structure",
      "members":{
        "message":{"shape":"NonBlankString"}
      },
      "documentation":"<p>The model specified in the request is not ready to serve inference requests.</p>",
      "error":{
        "httpStatusCode":429,
        "senderFault":true
      },
      "exception":true
    },
    "ModelStreamErrorException":{
      "type":"structure",
      "members":{
        "message":{"shape":"NonBlankString"},
        "originalStatusCode":{
          "shape":"StatusCode",
          "documentation":"<p>The original status code.</p>"
        },
        "originalMessage":{
          "shape":"NonBlankString",
          "documentation":"<p>The original message.</p>"
        }
      },
      "documentation":"<p>An error occurred while streaming the response. Retry your request.</p>",
      "error":{
        "httpStatusCode":424,
        "senderFault":true
      },
      "exception":true
    },
    "ModelTimeoutException":{
      "type":"structure",
      "members":{
        "message":{"shape":"NonBlankString"}
      },
      "documentation":"<p>The request took too long to process. Processing time exceeded the model timeout length.</p>",
      "error":{
        "httpStatusCode":408,
        "senderFault":true
      },
      "exception":true
    },
    "NonBlankString":{
      "type":"string",
      "pattern":"[\\s\\S]*"
    },
    "PartBody":{
      "type":"blob",
      "max":1000000,
      "min":0,
      "sensitive":true
    },
    "PayloadPart":{
      "type":"structure",
      "members":{
        "bytes":{
          "shape":"PartBody",
          "documentation":"<p>Base64-encoded bytes of payload data.</p>"
        }
      },
      "documentation":"<p>Payload content included in the response.</p>",
      "event":true,
      "sensitive":true
    },
    "ResourceNotFoundException":{
      "type":"structure",
      "members":{
        "message":{"shape":"NonBlankString"}
      },
      "documentation":"<p>The specified resource ARN was not found. Check the ARN and try your request again.</p>",
      "error":{
        "httpStatusCode":404,
        "senderFault":true
      },
      "exception":true
    },
    "ResponseStream":{
      "type":"structure",
      "members":{
        "chunk":{
          "shape":"PayloadPart",
          "documentation":"<p>Content included in the response.</p>"
        },
        "internalServerException":{
          "shape":"InternalServerException",
          "documentation":"<p>An internal server error occurred. Retry your request.</p>"
        },
        "modelStreamErrorException":{
          "shape":"ModelStreamErrorException",
          "documentation":"<p>An error occurred while streaming the response. Retry your request.</p>"
        },
        "validationException":{
          "shape":"ValidationException",
          "documentation":"<p>Input validation failed. Check your request parameters and retry the request.</p>"
        },
        "throttlingException":{
          "shape":"ThrottlingException",
          "documentation":"<p>The number or frequency of requests exceeds the limit. Resubmit your request later.</p>"
        },
        "modelTimeoutException":{
          "shape":"ModelTimeoutException",
          "documentation":"<p>The request took too long to process. Processing time exceeded the model timeout length.</p>"
        }
      },
      "documentation":"<p>Definition of content in the response stream.</p>",
      "eventstream":true
    },
    "ServiceQuotaExceededException":{
      "type":"structure",
      "members":{
        "message":{"shape":"NonBlankString"}
      },
      "documentation":"<p>The number of requests exceeds the service quota. Resubmit your request later.</p>",
      "error":{
        "httpStatusCode":400,
        "senderFault":true
      },
      "exception":true
    },
    "StatusCode":{
      "type":"integer",
      "box":true,
      "max":599,
      "min":100
    },
    "ThrottlingException":{
      "type":"structure",
      "members":{
        "message":{"shape":"NonBlankString"}
      },
      "documentation":"<p>The number of requests exceeds the limit. Resubmit your request later.</p>",
      "error":{
        "httpStatusCode":429,
        "senderFault":true
      },
      "exception":true
    },
    "Trace":{
      "type":"string",
      "enum":[
        "ENABLED",
        "DISABLED"
      ]
    },
    "ValidationException":{
      "type":"structure",
      "members":{
        "message":{"shape":"NonBlankString"}
      },
      "documentation":"<p>Input validation failed. Check your request parameters and retry the request.</p>",
      "error":{
        "httpStatusCode":400,
        "senderFault":true
      },
      "exception":true
    }
  },
  "documentation":"<p>Describes the API operations for running inference using Amazon Bedrock models.</p>"
}
